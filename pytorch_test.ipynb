{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0., 0., 0., 0., 0.])\n",
      "tensor([1., 2., 3., 4., 5.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a Tensor with just ones in a column\n",
    "a = torch.ones(5)\n",
    "print(a)\n",
    "\n",
    "# Create a Tensor with just zeros in a column\n",
    "b = torch.zeros(5)\n",
    "print(b)\n",
    "\n",
    "print(torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])) # custom valued tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[[1., 2.],\n",
      "         [3., 4.]],\n",
      "\n",
      "        [[5., 6.],\n",
      "         [7., 8.]]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.zeros(3,2))\n",
    "print(torch.ones(3,2))\n",
    "print(torch.tensor([[1.0, 2.0],[3.0, 4.0]]))\n",
    "\n",
    "# 3D Tensor\n",
    "g = torch.tensor([[[1., 2.], [3., 4.]], [[5., 6.], [7., 8.]]])\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# We can also find out the shape of a Tensor using .shape method.\n",
    "print(a.shape)\n",
    "print(g.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.)\n",
      "tensor(5.)\n",
      "tensor(5.)\n"
     ]
    }
   ],
   "source": [
    "# Access Tensor elements\n",
    "print(c[2])\n",
    "\n",
    "# All indices starting from 0\n",
    "\n",
    "# Get element at row 1, column 0 by any of these ways:\n",
    "print(g[1,0,0])\n",
    "print(g[1][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3.])\n",
      "tensor([1., 2., 3., 4.])\n",
      "tensor([1., 2.])\n",
      "tensor([2., 4.])\n"
     ]
    }
   ],
   "source": [
    "# How to access entire rows:\n",
    "\n",
    "# All elements\n",
    "print(g[:])\n",
    "\n",
    "# All elements from index 1 to 2 (inclusive)\n",
    "print(a[1:3])\n",
    "\n",
    "# All elements till index 4 (exclusive)\n",
    "print(a[:4])\n",
    "\n",
    "print(g[0,:])\n",
    "\n",
    "# Second column\n",
    "print(g[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "torch.float32\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "torch.int32\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------------------------------------\n",
    "#   Specify data type of elements\n",
    "#----------------------------------------------------------------------------------------------\n",
    "#   Whenever we create a tensor, PyTorch decides the data type of the elements of the tensor \n",
    "#   such that the data type can cover all the elements of the tensor. We can override this by \n",
    "#   specifying the data type while creating the tensor.\n",
    "\n",
    "int_tensor = torch.tensor([[1,2,3],[4,5,6]])\n",
    "print(int_tensor.dtype)\n",
    "\n",
    "# What if we changed any one element to floating point number?\n",
    "int_tensor = torch.tensor([[1,2,3],[4.,5,6]])\n",
    "print(int_tensor.dtype)\n",
    "print(int_tensor)\n",
    "\n",
    "# This can be overridden as follows\n",
    "int_tensor = torch.tensor([[1,2,3],[4.,5,6]], dtype=torch.int32)\n",
    "print(int_tensor.dtype)\n",
    "print(int_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 2.]\n",
      "  [3. 4.]]\n",
      "\n",
      " [[5. 6.]\n",
      "  [7. 8.]]]\n",
      "tensor([[8, 7, 6, 5],\n",
      "        [4, 3, 2, 1]])\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------------------------------------\n",
    "#   Tensor to/from NumPy Array\n",
    "#----------------------------------------------------------------------------------------------\n",
    "#\n",
    "# \tWe have mentioned several times that PyTorch Tensors and NumPy arrays are pretty similar. \n",
    "# \tThis raises the question if it’s possible to convert one data structure into another.\n",
    "# \tLet’s see how we can do this.\n",
    "\n",
    "# Import NumPy\n",
    "import numpy as np\n",
    "\t \n",
    "# Tensor to Array\n",
    "g_numpy = g.numpy()\n",
    "print(g_numpy)\n",
    "\n",
    "# Array to Tensor\n",
    "h = np.array([[8,7,6,5],[4,3,2,1]])\n",
    "h_tensor = torch.from_numpy(h)\n",
    "print(h_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  4,  0],\n",
      "        [ 8,  0, 12]])\n",
      "tensor([[ 0,  4,  0],\n",
      "        [ 8,  0, 12]])\n",
      "tensor([[ 2,  0,  6],\n",
      "        [ 0, 10,  0]])\n",
      "tensor([[ 2,  0,  6],\n",
      "        [ 0, 10,  0]])\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------------------------------------\n",
    "#\tArithmetic Operations on Tensors\n",
    "#----------------------------------------------------------------------------------------------\n",
    "#\n",
    "#\tNow it’s time for the next step. Let’s see how we can perform arithmetic operations on PyTorch tensors.\n",
    "\n",
    "# Create tensor\n",
    "tensor1 = torch.tensor([[1,2,3],[4,5,6]])\n",
    "tensor2 = torch.tensor([[-1,2,-3],[4,-5,6]])\n",
    "\t \n",
    "# Addition\n",
    "print(tensor1+tensor2)\n",
    "\n",
    "# We can also use\n",
    "print(torch.add(tensor1,tensor2))\n",
    "\n",
    "# Subtraction\n",
    "print(tensor1-tensor2)\n",
    "\n",
    "# We can also use\n",
    "print(torch.sub(tensor1,tensor2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  4,  6],\n",
      "        [ 8, 10, 12]])\n",
      "tensor([[ -1,   4,  -9],\n",
      "        [ 16, -25,  36]])\n",
      "tensor([[22, 28],\n",
      "        [49, 64]])\n"
     ]
    }
   ],
   "source": [
    "# Multiplication\n",
    "\n",
    "# Tensor with Scalar\n",
    "print(tensor1 * 2)\n",
    "\n",
    "# Tensor with another tensor\n",
    "# Elementwise Multiplication\n",
    "print(tensor1 * tensor2)\n",
    "\n",
    "# Matrix multiplication\n",
    "tensor3 = torch.tensor([[1,2],[3,4],[5,6]])\n",
    "print(torch.mm(tensor1,tensor3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 1],\n",
      "        [2, 2, 3]])\n",
      "tensor([[-1,  1, -1],\n",
      "        [ 1, -1,  1]])\n"
     ]
    }
   ],
   "source": [
    "# Division\n",
    "\n",
    "# Tensor with scalar\n",
    "print(tensor1/2)\n",
    "\n",
    "# Tensor with another tensor\n",
    "# Elementwise division\n",
    "print(tensor1/tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-4fc76b0a9c53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Create a tensor for GPU > This will occupy GPU RAM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtensor_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# If you are using Google Colab, focus on the RAM consumption meter in the top right corner and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyimage/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m         raise RuntimeError(\n\u001b[1;32m    161\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0m_cudart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyimage/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_isDriverSufficient'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_isDriverSufficient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getDriverVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------------------------------------\n",
    "#\tCPU v/s GPU Tensor\n",
    "#----------------------------------------------------------------------------------------------\n",
    "#\n",
    "#   PyTorch has different implementation of Tensor for CPU and GPU. \n",
    "#   Every tensor can be converted to GPU in order to perform massively parallel, fast computations. \n",
    "#   All operations that will be performed on the tensor will be carried out using GPU-specific \n",
    "#   routines that come with PyTorch. If you don’t have access to a GPU, you can perform these \n",
    "#   examples on Google Colab. Select GPU as Runtime. Let’s first see how to create a tensor for GPU.\n",
    "\n",
    "\n",
    "# Create a tensor for CPU > This will occupy CPU RAM\n",
    "tensor_cpu = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], device='cpu')\n",
    "\t \n",
    "# Create a tensor for GPU > This will occupy GPU RAM\n",
    "tensor_gpu = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], device='cuda')\n",
    "\n",
    "# If you are using Google Colab, focus on the RAM consumption meter in the top right corner and \n",
    "# you will see the GPU RAM consumption increase as soon as you create tensor_gpu.\n",
    "\n",
    "# Just like tensor creation, the operations performed for CPU and GPU tensors are also different and consume RAM corresponding to the device specified.\n",
    "\n",
    "# This uses CPU RAM\n",
    "tensor_cpu = tensor_cpu * 5\n",
    "\n",
    "# This uses GPU RAM > Focus on GPU RAM Consumption\n",
    "tensor_gpu = tensor_gpu * 5\n",
    "\n",
    "# The key point to note here is that no information flows to CPU in the GPU tensor operations (except if we print or access the tensor).\n",
    "# We can move the GPU tensor to CPU and vice versa as shown below.\n",
    "\n",
    "# Move GPU tensor to CPU\n",
    "tensor_gpu_cpu = tensor_gpu.to(device='cpu')\n",
    "\n",
    "# Move CPU tensor to GPU\n",
    "tensor_cpu_gpu = tensor_cpu.to(device='cuda')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('pyimage')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38660c56cb3cdba518e0c83d1a4eb9a88dbcee0b374732a9f0ec227e79ef154a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
